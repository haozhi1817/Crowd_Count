<!--
 * @Author: HaoZhi
 * @Date: 2022-10-17 09:23:35
 * @LastEditors: HaoZhi
 * @LastEditTime: 2022-10-17 16:04:01
 * @Description: 
-->
# CC Trans
本代码参考自github相关CCTrans代码以及DMCount相关代码。

## 数据分布

| 数据集 | 数量 | 分布 |
| --- | --- | --- |
|参数训练集 | 898 | 12~2672, 集中在500以内 |
|参赛测试集 | 300 | - |
|shanghaiA训练集|300| 33 ~ 3138, 集中在700以内 |
|shanghaiB训练集|400| 12~576, 集中在200以内 |

## 实验记录

| 数据集 | 模型结构 | 损失函数 | MAE |
| --- | --- | --- | --- |
|参赛训练集 | EfficientNet | 4分支回归 | 187 |
|参赛训练集 | EfficientNet | 50分支分布回归 | 184 |
| shanghaiB | VGG | count_loss + ot_loss + tv_loss | 87 | 67 |
| shanghaiA + fine_tune(shanghaiB) | VGG | count_loss + ot_loss + tv_loss | 

## 结果分析

* 使用pixel-wise方式去建模要比直接做整图回归的精度好很多。
* 在利用上海B的参数训练上海A时，发现上个数据集分布似乎不太一致，上海A收敛的没有上海B好，但是就结果而言，使用额外的数据集效果更好，同时上海A中包含很多超过1000的数据，可能这也是效果变好的原因之一。
* OT_Loss可能存在负数，几乎很难收敛网上关于这一块儿的讲解很少，关于skinhorn解法中beta的使用规则也没有详细说明，但是感觉这个方案其实是可行的，需要继续研究。

## OT_Loss

DM-Count中关于beta的计算参考[知乎](https://www.zhihu.com/question/436408717)， 理论上应该是reg * (log(v) + 0.5), 这里缺少0.5. 
这里的V代表一个约束，使得矩阵P(shape = (M * N))的np.sum(P, axis = 1) = N, 在本实验中即保证P的行向量之和为N，也就是标签dot的个数。
我们这里讨论ot_loss，首先将其定义为r，c两个分布的距离，如DM-count论文中有：

$$l_{ot} = w(r, c)$$

其中r代表64 * 64个特征， c代表4个目标，这两个tensor都被归一化，使得他们均为一个“分布”，首先我们有代价矩阵M(4,4096),代价矩阵中的每一个点代表该点到某一目标点的l2距离。代价矩阵与r无关，也就是说，无论模型预测出的特征是什么样子，M都是固定的。我们需要利用这个M以及r与c，计算一个矩阵P，使得P * M最小。这个矩阵P就是根据M，r, c得到的转移矩阵，这个矩阵代表着将r转移为c时，使得P * M最小的一种方案，而P * M则为r与c在代价矩阵M的基础上的距离。这里出现了一个非常强的悖论，即我们第一步，需要固定M，r，c来计算矩阵P，第二步，我们需要固定P，M，c，来让模型优化r。已知第一步的目标函数是P * M最小，那么第二步显然不能用这个来做目标函数。
已知：
$$P = u * M * v = \alpha * \beta * \exp(-\lambda * M)$$

这里的v或者$\beta$二者的shape = 4096，他是对r的一种约束，可以理解为贡献矩阵，使得$\sum_{i} r_{i} * \beta_{i} = 0.25(也就是normed(c))$同时满足$\sum_{i} M_{i,j}\beta_{i}$最小。同理$\alpha$意义类似。作者在这里将距离函数拆分为：
$$l_{ot} = w(r, c) = \langle \alpha, c \rangle + \langle \beta, r \rangle $$
说实话，这个拆解我并不理解，一来我没有推导出这么拆解的数学逻辑，因为很明显$\beta$的计算过程与$\alpha$是耦合在一起的。更重要的是，无论是否拆解，都不能解决我们上述提到的悖论。
总之，作者在这里通过loss拆解与求导，最终只保留了后者作为loss，说实话，我不是很理解，$\langle \beta, r \rangle$代表的数学意义是什么，我们为什么要最小化这个东西？

另外关于OT_loss存在负数的问题，我们根据论文逐一推导，首先：
$$loss = \frac{\beta}{||z||_{1}} - \frac{\langle \beta, z \rangle}{||z||^{2}_{1}} = \frac{\beta * ||z||_{1} - \langle \beta, z \rangle}{||z||^{2}_{1}} = \frac{\sum \beta_{i} * ||z||_{1}-\sum\beta_{i} * z_{i}}{||z|^{2}_{1}|} = \frac{\sum\beta_i * (||z||_{1} - z_{i})}{||z||^{2}_{1}}$$
由上式可知，如果loss出现负数，只可能是分子中的两项出现了负数，首先$\beta$由log计算而来，存在较多的负数，另一方面当z中不存在负数时，必然有$||z||_{1} > z_{i}$。这提示我们，首先z至少需要经过relu激活，因为z在数学上本就不应该出现负值。这一点在代码中已有保证，那么问题就出现在$\beta$上。已知v必为正数（因为他是在一堆正数除法中得到的），但是他代表着r对$c_{i}$d的贡献，当c非常离散或者分布不均匀时，r的贡献矩阵$\beta$中势必存在大量的0，这就对应着$\beta$中势必存在大量负数，且不是一个0.5可以矫正的，这也是为什么代码中$\lambda$足够大的原因，即使得P尽可能均匀，减小0出现的概率。因此，从这个点入手，一方面，可以加大$\lambda$, 另一方面减小crop_size，或者，直接使用v而非$\beta$, 但是v的数值特别小，难办，难道只能调整crop_size以及$\lambda$?后续还需要继续分析。